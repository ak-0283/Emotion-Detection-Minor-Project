{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rYGA3rRLOsOU0y2cmqcCv1InPwE3gp6U","authorship_tag":"ABX9TyNwNg21zyJMxW49uD+8+CI6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"UUwpCfxIwRX5","executionInfo":{"status":"ok","timestamp":1762871393383,"user_tz":-330,"elapsed":11054,"user":{"displayName":"abc def","userId":"14539853576615368447"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8462cad2-d086-48fc-d0ea-816cabf857af"},"outputs":[{"output_type":"stream","name":"stdout","text":["âš ï¸ No GPU detected â€” training may be slow.\n"]}],"source":["# --- Core imports ---\n","import os, random, pickle, time\n","import numpy as np\n","import tensorflow as tf\n","from pathlib import Path\n","\n","# --- Keras imports ---\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications import (\n","    VGG16, ResNet50, InceptionV3, EfficientNetB0\n",")\n","from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n","from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n","from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n","from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n","\n","# --- GPU setup ---\n","gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n","if gpus:\n","    for g in gpus:\n","        tf.config.experimental.set_memory_growth(g, True)\n","    print(\"âœ… GPU detected and configured.\")\n","else:\n","    print(\"âš ï¸ No GPU detected â€” training may be slow.\")\n","\n","random.seed(42)\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","# Adjust path if needed\n","dataset_root = Path(\"/content/drive/MyDrive/24_images\")\n","train_dir = dataset_root / \"train\"\n","val_dir   = dataset_root / \"test\"\n","\n","print(\"Train:\", train_dir)\n","print(\"Test :\", val_dir)\n","\n","assert train_dir.exists(), f\"âŒ Train folder not found: {train_dir}\"\n","assert val_dir.exists(), f\"âŒ Test folder not found: {val_dir}\"\n","\n","train_count = sum(len(f) for _,_,f in os.walk(train_dir))\n","val_count   = sum(len(f) for _,_,f in os.walk(val_dir))\n","print(f\"âœ… Dataset OK â€” Train: {train_count},  Test: {val_count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22tJj8TowVRu","executionInfo":{"status":"ok","timestamp":1762871424317,"user_tz":-330,"elapsed":18872,"user":{"displayName":"abc def","userId":"14539853576615368447"}},"outputId":"773d3c8a-a225-475a-b4f7-d27b80b1b89f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Train: /content/drive/MyDrive/24_images/train\n","Test : /content/drive/MyDrive/24_images/test\n","âœ… Dataset OK â€” Train: 168,  Test: 168\n"]}]},{"cell_type":"code","source":["BATCH_SIZE = 8\n","\n","MODEL_SPECS = {\n","    \"VGG16\": {\"base\": VGG16, \"target_hw\": (224, 224), \"preprocess\": vgg_preprocess},\n","    \"ResNet50\": {\"base\": ResNet50, \"target_hw\": (224, 224), \"preprocess\": resnet_preprocess},\n","    \"InceptionV3\": {\"base\": InceptionV3, \"target_hw\": (299, 299), \"preprocess\": inception_preprocess},\n","    \"EfficientNetB0\": {\"base\": EfficientNetB0, \"target_hw\": (224, 224), \"preprocess\": effnet_preprocess},\n","}\n"],"metadata":{"id":"swdvnCM_wsBO","executionInfo":{"status":"ok","timestamp":1762871428009,"user_tz":-330,"elapsed":22,"user":{"displayName":"abc def","userId":"14539853576615368447"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Create data generators per model\n","def make_datagens(preprocess_fn):\n","    train_datagen = ImageDataGenerator(\n","        preprocessing_function=preprocess_fn,\n","        rotation_range=15,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1,\n","        horizontal_flip=True\n","    )\n","    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_fn)\n","    return train_datagen, val_datagen\n","\n","\n","# Build CNN model for each backbone â€” clean, NO Lambda layers\n","def build_emotion_model(name, num_classes):\n","    spec = MODEL_SPECS[name]\n","    target_h, target_w = spec[\"target_hw\"]\n","    base_cls = spec[\"base\"]\n","\n","    inp = Input(shape=(target_h, target_w, 3), name=f\"{name}_input\")\n","    base_model = base_cls(weights=\"imagenet\", include_top=False, input_tensor=inp)\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    x = GlobalAveragePooling2D()(base_model.output)\n","    x = Dense(128, activation=\"relu\")(x)\n","    x = Dropout(0.4)(x)\n","    out = Dense(num_classes, activation=\"softmax\")(x)\n","\n","    model = Model(inputs=base_model.input, outputs=out, name=f\"{name}_emotion_model\")\n","    return model\n"],"metadata":{"id":"5STWLk18wuLm","executionInfo":{"status":"ok","timestamp":1762871430236,"user_tz":-330,"elapsed":7,"user":{"displayName":"abc def","userId":"14539853576615368447"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["save_dir = Path(\"/content/drive/MyDrive/EmotionDetection_Trained\")\n","save_dir.mkdir(parents=True, exist_ok=True)\n","\n","for name, spec in MODEL_SPECS.items():\n","    print(f\"\\n==============================\")\n","    print(f\"Training {name} model\")\n","    print(f\"==============================\")\n","\n","    preprocess_fn = spec[\"preprocess\"]\n","    target_h, target_w = spec[\"target_hw\"]\n","\n","    train_datagen, val_datagen = make_datagens(preprocess_fn)\n","\n","    train_data = train_datagen.flow_from_directory(\n","        train_dir,\n","        target_size=(target_h, target_w),\n","        batch_size=BATCH_SIZE,\n","        class_mode=\"categorical\",\n","        shuffle=True\n","    )\n","\n","    val_data = val_datagen.flow_from_directory(\n","        val_dir,\n","        target_size=(target_h, target_w),\n","        batch_size=BATCH_SIZE,\n","        class_mode=\"categorical\",\n","        shuffle=False\n","    )\n","\n","    num_classes = len(train_data.class_indices)\n","    emotion_labels = list(train_data.class_indices.keys())\n","\n","    model = build_emotion_model(name, num_classes)\n","    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n","                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    start = time.time()\n","    model.fit(train_data, validation_data=val_data, epochs=3, verbose=1)\n","    print(f\"â± Training time: {(time.time()-start):.2f} sec\")\n","\n","    model_path = save_dir / f\"{name}_emotion_model.keras\"\n","    model.save(model_path)\n","    print(f\"ğŸ’¾ Saved model: {model_path}\")\n","\n","    metadata = {\n","        \"model_name\": name,\n","        \"emotion_labels\": emotion_labels,\n","        \"class_indices\": train_data.class_indices\n","    }\n","    with open(save_dir / f\"{name}_metadata.pkl\", \"wb\") as f:\n","        pickle.dump(metadata, f)\n","    print(f\"ğŸ§¾ Saved metadata: {save_dir / f'{name}_metadata.pkl'}\")\n","\n","print(f\"\\nâœ… All models and metadata saved to: {save_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U0xq10BQwvtG","executionInfo":{"status":"ok","timestamp":1762872858643,"user_tz":-330,"elapsed":1422762,"user":{"displayName":"abc def","userId":"14539853576615368447"}},"outputId":"9fdd606b-ed4b-4891-da86-5d02eb2f55d9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==============================\n","Training VGG16 model\n","==============================\n","Found 168 images belonging to 7 classes.\n","Found 168 images belonging to 7 classes.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m58889256/58889256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 12s/step - accuracy: 0.0995 - loss: 5.5255 - val_accuracy: 0.1310 - val_loss: 3.6248\n","Epoch 2/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 11s/step - accuracy: 0.1203 - loss: 4.3173 - val_accuracy: 0.1250 - val_loss: 3.1114\n","Epoch 3/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 12s/step - accuracy: 0.1303 - loss: 4.0385 - val_accuracy: 0.1607 - val_loss: 2.8084\n","â± Training time: 718.71 sec\n","ğŸ’¾ Saved model: /content/drive/MyDrive/EmotionDetection_Trained/VGG16_emotion_model.keras\n","ğŸ§¾ Saved metadata: /content/drive/MyDrive/EmotionDetection_Trained/VGG16_metadata.pkl\n","\n","==============================\n","Training ResNet50 model\n","==============================\n","Found 168 images belonging to 7 classes.\n","Found 168 images belonging to 7 classes.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.1143 - loss: 2.5783 - val_accuracy: 0.1071 - val_loss: 2.1505\n","Epoch 2/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 4s/step - accuracy: 0.2342 - loss: 2.1301 - val_accuracy: 0.1131 - val_loss: 2.0682\n","Epoch 3/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 4s/step - accuracy: 0.1876 - loss: 2.1846 - val_accuracy: 0.1190 - val_loss: 2.0567\n","â± Training time: 229.29 sec\n","ğŸ’¾ Saved model: /content/drive/MyDrive/EmotionDetection_Trained/ResNet50_emotion_model.keras\n","ğŸ§¾ Saved metadata: /content/drive/MyDrive/EmotionDetection_Trained/ResNet50_metadata.pkl\n","\n","==============================\n","Training InceptionV3 model\n","==============================\n","Found 168 images belonging to 7 classes.\n","Found 168 images belonging to 7 classes.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m87910968/87910968\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 5s/step - accuracy: 0.1698 - loss: 2.1570 - val_accuracy: 0.1310 - val_loss: 2.0143\n","Epoch 2/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 5s/step - accuracy: 0.1924 - loss: 2.0700 - val_accuracy: 0.1190 - val_loss: 2.0090\n","Epoch 3/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 7s/step - accuracy: 0.1286 - loss: 2.0409 - val_accuracy: 0.1250 - val_loss: 1.9695\n","â± Training time: 341.64 sec\n","ğŸ’¾ Saved model: /content/drive/MyDrive/EmotionDetection_Trained/InceptionV3_emotion_model.keras\n","ğŸ§¾ Saved metadata: /content/drive/MyDrive/EmotionDetection_Trained/InceptionV3_metadata.pkl\n","\n","==============================\n","Training EfficientNetB0 model\n","==============================\n","Found 168 images belonging to 7 classes.\n","Found 168 images belonging to 7 classes.\n","Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n","\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.1237 - loss: 2.1731 - val_accuracy: 0.1012 - val_loss: 2.0105\n","Epoch 2/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.1186 - loss: 2.0571 - val_accuracy: 0.1548 - val_loss: 1.9813\n","Epoch 3/3\n","\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.1367 - loss: 2.0353 - val_accuracy: 0.1310 - val_loss: 1.9743\n","â± Training time: 103.21 sec\n","ğŸ’¾ Saved model: /content/drive/MyDrive/EmotionDetection_Trained/EfficientNetB0_emotion_model.keras\n","ğŸ§¾ Saved metadata: /content/drive/MyDrive/EmotionDetection_Trained/EfficientNetB0_metadata.pkl\n","\n","âœ… All models and metadata saved to: /content/drive/MyDrive/EmotionDetection_Trained\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.utils import load_img, img_to_array\n","\n","test_img_path = \"/content/drive/MyDrive/24_images/test/angry/PrivateTest_1054527.jpg\"\n","\n","if os.path.exists(test_img_path):\n","    for name, spec in MODEL_SPECS.items():\n","        print(f\"\\n==============================\")\n","        print(f\"Predicting with {name} model\")\n","        print(f\"==============================\")\n","        target_h, target_w = spec[\"target_hw\"]\n","\n","        img = load_img(test_img_path, target_size=(target_h, target_w))\n","        arr = img_to_array(img)\n","        arr = np.expand_dims(arr, 0)\n","\n","        model_path = save_dir / f\"{name}_emotion_model.keras\"\n","        meta_path = save_dir / f\"{name}_metadata.pkl\"\n","\n","        model = tf.keras.models.load_model(model_path, compile=False)\n","        with open(meta_path, \"rb\") as f:\n","            meta = pickle.load(f)\n","\n","        # Apply preprocessing function specific to the model\n","        if \"preprocess\" in spec and spec[\"preprocess\"] is not None:\n","          arr = spec[\"preprocess\"](arr)\n","\n","        preds = model.predict(arr, verbose=0)\n","        label = meta[\"emotion_labels\"][np.argmax(preds)]\n","        print(f\"{name}: {label}\")\n","else:\n","    print(\"âš ï¸ Please update test_img_path.\")"],"metadata":{"id":"7rRKHTifw3ZW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762874270233,"user_tz":-330,"elapsed":17631,"user":{"displayName":"abc def","userId":"14539853576615368447"}},"outputId":"ac91622d-a657-4547-89de-7d10ceb41238"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==============================\n","Predicting with VGG16 model\n","==============================\n","VGG16: disgust\n","\n","==============================\n","Predicting with ResNet50 model\n","==============================\n","ResNet50: fear\n","\n","==============================\n","Predicting with InceptionV3 model\n","==============================\n","InceptionV3: angry\n","\n","==============================\n","Predicting with EfficientNetB0 model\n","==============================\n","EfficientNetB0: disgust\n"]}]}]}